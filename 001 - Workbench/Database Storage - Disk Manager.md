- Course Outline
	- [[Query Planning]]
	- [[Operator Execution]]
	- [[Access Methods]]
	- [[Buffer Pool Manager]]
	- ðŸ‘‰ Disk Manager

- move from volatile and non-volatile storage
	- Volatile (Random Access Byte-Addressable), a directly storage
		- CPU reg
		- CPU caches
		- --- ðŸ‘† 15-721
		- DRAM - 100ns
	- Non-Volatile (Sequential Access Block-Addressable), pick pieces of blocks before storage
		- SSD - 150000ns
		- HDD
		- Network Storage
	- try to maximize the amount of data we can read that's sequential
	- cache to avoid a long time read
	- Exec Engine do quern on Buffer Pool (small than disk), and Buffer Pool do query on disk
- `mmap` to store contents of file into process' address space, and OS responsible for moving data in and out of memory
	- ![[Pasted image 20220515101915.png | 300]]
	- if we need `write` rather than `read`, things become more complicated, OS need to know which page can be flushed to disk
		- this is the problem that to be dealt with especially for concurrency
		- hints
			- `madvise`
			- `mlock`
			- `msync`


How represents the Data in files on disk
- storage manager
	- schedule for reads and writes to improve spatial and temporal locality of pages
	- organizes files as a collection of pages
		- tracks data read/written
		- tracks the available spaces
- pages
	- a page is fix size of block of data
		- contains tuples and meta-data and etc.
		- *do not mix page types*
		- *require a page is self-contained*
	- unique page ID
		- use an layer to map page ids to physical locations
	- page size (guaranteed by hardware)
		- illustrate the time that the size can be handled with an **atomic** operation -> tolerance of truncating data
		- (usually)
		- hardware -> 4K
		- OS -> 4K
		- database -> 512B - 16KB
		- (my understanding is) If the high-level want to use a larger page than the bottom-level provides, it needs addition overhead mitigation measure to ensure failsafe of `writing` (on contrary, you can read more sequential data with a single query and of course, `read` is safer than `write`), to prevent from `writing` queues out of order and getting a unexpected result.
			- maybe **checksum** in each page for a faster matching, if any errors found, system will use **logs** to recover a particular transaction.
	- architecture
		- heap file
			- outline
				- unordered collection
				- c / g / w / d
				- support iterating
			- track free space
			- ways to implement it
				- linked list
					- HEADER for DATA and FREEPAGE
					- each page keep track the number of free slots itself
					- pic![[Pasted image 20220515113426.png | 200]]
				- page directory
					- pic ![[Pasted image 20220515113753.png | 150]]
					- it need keep sync of the location and metadata of page inside a directory
	- page consists of **header** and data
		- page size
		- checksum
		- DBMS version
		- transaction visibility
		- compression information
	- page layout (how page organize data)
		- tuple-oriented
			- keep tuples in sequence (BAD idea)
				- insert tuple one after another, and track the number of tuples
				- ~~**if you need to delete a tuple, you should move everything, or leave with a external fragmentation which can not be tracked**~~
				- (correction) what he want to illustrate here is to demonstrate that a it have **serious drawbacks** if we just track the number of tuples, since the high-level needs to care about where the tuple it need are put. (if some tuple was deleted)
			- slotted pages
				- pic ![[Pasted image 20220515134440.png | 150]] 
				- map slots to the **offset** of each tuples
				- header keep track of # (slots) and the length of slots array
				- grow from beginning to end, and end to beginning
		- only the tuples of a same table will go to the same page
		- record tuples
			- `page_id + offset/slot`
			- also a file location info
			- and use these info to find page and slot of tuple
	- tuple layout
		- visibility info for concurrency control
		- bit map for `NULL` values
		- store high-level metadata (and low-level such as column info is useless)
		- tuple data
			- typically stored in order
		- THE REASON that only same table of tuples will go to the same page is named DENORMALIZED
			- we could simply concatenate (`pre join`) two table with adding a new field of metadata to prevent from expensive updates
			- BUT, it will cause amount of I/O for common workload patterns, especially for reading
			- so, it needs a trade-off when you design the system itself.
	- log structure
		- store whole log how it constructed
		- update fast compare of update cross page, just append is enough
		- read is slow
		- compact periodically to increase speed with ID
	- catalog
		- ieee-754
		- CPU instructor not accurate, but system side is slow
		- type affect speed real 2x than decimal
		- fix point decimal to ensure data not lose
		- metadata
		- most store in it self
		- ASCII INFORMATION_SCHEMA
			- but systems have own shortcuts to access it
		- use live compile to make interpret catalog faster
	- large value
		- overflow **storage page** that value point to VARCHAR DATA
		- or **external file storage** but can not be manipulate and no protection
		- store cache thumbnail to database up to 1mb may faster load
	- Online Query
		- OLTP transaction row update
		- OLAP analytical column query
		- HTAP same database instance
	- N-ARY column
		- use slot to store a bunch of continual data
		- compare with row, fast insert, update and remove, good for query **tuple**
		- disadvantage is large portion of data scan is slow, can hard to get subset of attributes
		- basically the storage structure is different
	- decomposition store module (store all in one table)
		- store
			- store tuple id each entity
			- or use a same offset
		- column
		- vs to N-ARY -> no index of slot? but save memory
	- OLTP (frontend) and some time after stream to OLAP Backend Database
	- buffer pool <-> execution server
		- take **page** from disk to memory fixed-size **frame** (array entry of buffer pool) then access it
		- pagetable <-> buffer pool	
			- page in use
			- metadata
			- dirty flag
			- pin/ref counter
			- **latch** before update pagetable
	- lock vs latches
		 - latch: protecting internal critical section, no need to rollback, actually **mutex** spin lock
		 - lock: protect logical content, need to rollback
	- page directory vs page table
		- directory: id <-> database, durable
		- table: page <-> buffer pool, in-memory, not durable
	- global policies vs local policies
		- local: particular query like for single buffer pool
	- buffer pool opti:
		- multiple buffer pools
			- different pool different polices opti for diff workload
			- thread not contend -> but still depend on disk
			- hash and mod for a record to log
		- pre fetching
			- based on a query plan and prefetching a bunch of **pages**
		- scan sharing
			- reuse one page for one thread and another at diff query
			- intermediate result
			- second query cursor -> keep tracking where the second query joined with the first
			- q2 follow q1 using a occupied slot of buffer pool, finally scan the remaining ignore at the start
				- database is unordered
		buffer pool <-> disk page, replacement
		- buffer pool bypass
			- avoid buffer pool overhead
			- not huge thing
			- contiguous thing read on disk may good
		- bypass OS side cache, use DBMS direct IO it self
			- redundant copies of page (yours and OSs) for cross system using
			- eviction policy
			- not to cache anything from system
	- buffer replacement policies
		- correctness
		- accuracy
		- speed
		- meta-data overhead
		- strategy
			- LRU
				- use CLOCK to approximate it
				- a reference bit indicate if access since last check
				- date is put like a circled ring
				- if the accessed bit is set, remove it at the next time the clock tick it
			- Easy to get sequential flooding
				- read a bunch of page with newer timestamp that CANNOT be evicted
				- those page maybe one-time use
				- can not evict page effectively
			-